{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a44bd8d9-e04c-4ad4-9457-577a1f302c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d6a01-08cf-4433-bad6-ff50b909c726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2de1b73-0f68-4ed8-9094-0790a65f7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mamba(t.nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        rms_norm = t.nn.RMSNorm(list(input_shape))\n",
    "        \n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = t.nn.Conv2d(1, 6, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.L1 = t.nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.L2 = t.nn.Linear(120, 84)\n",
    "        self.L3 = t.nn.Linear(84, 10)\n",
    "        self.L4 = t.nn.Linear(84, 10)\n",
    "\n",
    "        #Activation function \n",
    "        self.SiLU = t.nn.SiLU()\n",
    "\n",
    "        self.SoftMax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, u):\n",
    "\n",
    "        #MAMBA block starts \n",
    "        \n",
    "        x = rms_norm(u)\n",
    "\n",
    "        z = self.L1(x)\n",
    "\n",
    "        x = self.L2(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.SiLU(x)\n",
    "\n",
    "        z = self.SiLU(x) \n",
    "\n",
    "        x, final_state = self.SSD(x, A, B, C)\n",
    "\n",
    "        x = t.matmul(x, z)\n",
    "\n",
    "        x = self.L3(x) + u \n",
    "\n",
    "        #MAMBA block ends \n",
    "\n",
    "        x = rms_norm(x)\n",
    "\n",
    "        x = self.L4(x)\n",
    "\n",
    "        x = SoftMax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def segsum(x):\n",
    "        \"\"\"Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix,\n",
    "        which is equivalent to a scalar SSM.\"\"\"\n",
    "        T = x.size(-1)\n",
    "        x_cumsum = t.cumsum(x, dim=-1)\n",
    "        x_segsum = x_cumsum[..., :, None] - x_cumsum[..., None, :]\n",
    "        mask = t.tril(t.ones(T, T, device=x.device, dtype=bool), diagonal=0)\n",
    "        x_segsum = x_segsum.masked_fill(~mask, -t.inf)\n",
    "        return x_segsum\n",
    "    \n",
    "    def ssd(X, A, B, C, block_len=64, initial_states=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        X: (batch, length, n_heads, d_head)\n",
    "        A: (batch, length, n_heads)\n",
    "        B: (batch, length, n_heads, d_state)\n",
    "        C: (batch, length, n_heads, d_state)\n",
    "        Return:\n",
    "        Y: (batch, length, n_heads, d_head)\n",
    "        \"\"\"\n",
    "        assert X.dtype == A.dtype == B.dtype == C.dtype\n",
    "        assert X.shape[1] % block_len == 0\n",
    "        # Rearrange into blocks/chunks\n",
    "        X, A, B, C = [rearrange(x, \"b (c l) ... -> b c l ...\", l=block_len) for x in (X, A, B, C)]\n",
    "        A = rearrange(A, \"b c l h -> b h c l\")\n",
    "        A_cumsum = t.cumsum(A, dim=-1)\n",
    "        # 1. Compute the output for each intra-chunk (diagonal blocks)\n",
    "        L = t.exp(segsum(A))\n",
    "        Y_diag = t.einsum(\"bclhn,bcshn,bhcls,bcshp->bclhp\", C, B, L, X)\n",
    "        # 2. Compute the state for each intra-chunk\n",
    "        # (right term of low-rank factorization of off-diagonal blocks; B terms)\n",
    "        decay_states = t.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n",
    "        states = t.einsum(\"bclhn,bhcl,bclhp->bchpn\", B, decay_states, X)\n",
    "        # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries\n",
    "        # (middle term of factorization of off-diag blocks; A terms)\n",
    "        if initial_states is None:\n",
    "            initial_states = t.zeros_like(states[:, :1])\n",
    "        states = t.cat([initial_states, states], dim=1)\n",
    "        decay_chunk = t.exp(segsum(F.pad(A_cumsum[:, :, :, -1], (1, 0))))\n",
    "        new_states = t.einsum(\"bhzc,bchpn->bzhpn\", decay_chunk, states)\n",
    "        states, final_state = new_states[:, :-1], new_states[:, -1]\n",
    "        # 4. Compute state -> output conversion per chunk\n",
    "        # (left term of low-rank factorization of off-diagonal blocks; C terms)\n",
    "        state_decay_out = t.exp(A_cumsum)\n",
    "        Y_off = t.einsum('bclhn,bchpn,bhcl->bclhp', C, states, state_decay_out)\n",
    "        # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)\n",
    "        Y = rearrange(Y_diag+Y_off, \"b c l h p -> b (c l) h p\")\n",
    "        return Y, final_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66e41fb7-febd-4268-9684-e099126af891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9432, -1.0047, -0.1954],\n",
       "         [-0.4737,  1.8013, -0.7704]],\n",
       "\n",
       "        [[-0.0681, -1.3904, -1.0567],\n",
       "         [-0.8493, -1.2979,  0.7348]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms_norm = t.nn.RMSNorm([2, 3])\n",
    "Input = t.randn(2, 2, 3)\n",
    "rms_norm(Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c9f46-35f1-46e3-8eb7-8930a90467f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fc64951-8b64-4a1b-b23b-b6a9c2f02d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8739, -0.9310, -0.1811],\n",
       "        [-0.4389,  1.6691, -0.7138]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9dcadfa-d8f9-4057-b65a-74b2cba4ccf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace4a80a-4600-493e-8ad7-07c2040e8a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49352811-5330-47c0-8bc1-8defec6d6a41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74ddd9-2064-4f22-8347-0efd33550c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6601d5e9-44e8-4ab7-bef2-451fc476b65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef243a8-5404-4153-8d25-41ec7802dcde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
